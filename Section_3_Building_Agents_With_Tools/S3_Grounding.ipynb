{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson: Grounding in Reality with External Information\n",
        "\n",
        "Welcome to this lesson on **Grounding**. This is how you connect your AI to sources of real-time, factual information, transforming it from a knowledgeable LLM into an up-to-date expert.\n",
        "\n",
        "### The Concept\n",
        "Grounding is the process of connecting the Gemini model to an external information source to ensure its answers are factual and current. There are two primary ways to do this:\n",
        "1.  **Google Search:** Allows the model to search the web for up-to-the-minute information.\n",
        "2.  **URL Context:** Forces the model to base its answers on the content of a specific webpage you provide.\n",
        "\n",
        "In this notebook, we'll explore both methods and even combine them."
      ],
      "metadata": {
        "id": "3j4dnYAepes3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp4-iNEFpDaN",
        "outputId": "88b3e213-9853-4bf1-aacc-d07137cf187c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.2/236.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.15.1 requires google-genai!=1.37.0,!=1.38.0,!=1.39.0,<=1.40.0,>=1.21.1, but you have google-genai 1.42.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title 1. Setup\n",
        "# Install the Google AI Python SDK\n",
        "!pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Configure your API Key\n",
        "# Use the \"Secrets\" tab in Colab (click the key icon on the left) to store your\n",
        "# API key with the name \"GOOGLE_API_KEY\".\n",
        "\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "except userdata.SecretNotFoundError as e:\n",
        "    print('Secret not found. Please add your GOOGLE_API_KEY to the Colab Secrets Manager.')"
      ],
      "metadata": {
        "id": "N-Zq0U2TpqNX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: The Problem - A Model Without Grounding\n",
        "\n",
        "First, let's see what happens when we ask a standard model a question about a very recent event. Its knowledge will be outdated."
      ],
      "metadata": {
        "id": "Sa26YT0ep0yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a time-sensitive question.\n",
        "from IPython.display import Markdown, display, HTML\n",
        "prompt = \"Who won the Nobel Prize in Physics for the year 2025?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents = prompt\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7ByWnj9gpztX",
        "outputId": "9918b6c0-a8c7-43cf-834f-0bb07f5acc55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Nobel Prize in Physics for the year 2025 has not been announced yet.\n\nThe Nobel Prizes are typically announced in October of the award year. We will find out the 2025 laureates in October 2025."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Grounding with Google Search\n",
        "\n",
        "Now, let's give our model the power of Google Search. We do this by passing `config={\"tools\": [{\"google_search\": {}}]},` to the model."
      ],
      "metadata": {
        "id": "T0u8siqwq84Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents = prompt,\n",
        "    config={\"tools\": [{\"google_search\": {}}]}\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "print(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n",
        "print(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n",
        "\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tkQ_YIamrPMJ",
        "outputId": "2452d837-c166-4a94-b9e6-fdce20d32d41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Nobel Prize in Physics for the year 2025 has been awarded to John Clarke, Michel H. Devoret, and John M. Martinis. The Royal Swedish Academy of Sciences in Stockholm announced the laureates on October 7, 2025.\n\nThese scientists were recognized for their groundbreaking work on macroscopic quantum phenomena in electrical circuits. Their pioneering experiments demonstrated quantum tunneling and energy quantization on a macroscopic scale, significantly advancing the understanding of quantum mechanics in engineered systems. This research is also noted for paving the way for advanced quantum technology, including quantum computing."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Query: ['Nobel Prize in Physics 2025 announcement date', 'when are Nobel Prizes announced 2025']\n",
            "Search Pages: nobelprize.org, vajiramandravi.com, aljazeera.com, britannica.com, time.com\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZGdopvMzsYLv2C5QNIvmotucZoMDf1ETovYrN9_f76tSD93cw0qMUZE0WhyRYR1ER-8uLdOX14u_AdXB0uh7q-O_hjRQqt1EjNWEjzONOlKcnKfDwDjWg1jJnxJ-RjMTCEIt2EjV2qrG4IuaAelTumYtyk3946P2CzJTLsBczjpB_FJzU1Tvn_kHaL1BycxIPQjmp1OYo_MMSKWFvAhUvDWe-Ymbu\">when are Nobel Prizes announced 2025</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZrI3-F6owybqpnRzc2yNlyPvgs0S2Bu30NGc-_26fguaKTK7cvR23JE1FlbeUTFNFwMTdDj6w0unpwbQfdlJW6xZga4QkRLDTHAyGhEKfzC_oiIf0kPHAnpHkFizxxqn5iv9xBb-fmLQ1LvnUXpezX8cjpwUYXtCmCEita2l1ccsqRe2-kSVWkKiI_CiabHchgW-1KFhO9CRRLAnN7JdCbD-0JPUKJYMeda8IYYe3\">Nobel Prize in Physics 2025 announcement date</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Grounding with a Specific URL\n",
        "\n",
        "What if you want the model to answer questions based *only* on a specific document or webpage? You can provide a URL as the grounding source. This is perfect for building a \"Document Q&A\" bot.\n",
        "\n",
        "The URL Context tool empowers Gemini models to directly access and process content from specific web page URLs you provide within your API requests. This is incredibly interesting because it allows your applications to dynamically interact with live web information without needing you to manually pre-process and feed that content to the model.\n",
        "\n",
        "URL Context is effective because it allows the models to base its responses and analysis directly on the content of the designated web pages. Instead of relying solely on its general training data or broad web searches (which are also valuable grounding tools), URL Context anchors the model's understanding to the specific information present at those URLs.\n",
        "\n",
        "Let's create an expert on the James Webb Space Telescope using its Wikipedia page."
      ],
      "metadata": {
        "id": "vqqbjXtRsjQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Based on https://en.wikipedia.org/wiki/James_Webb_Space_Telescope, what are the four key scientific instruments on the James Webb Space Telescope?\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents = prompt,\n",
        "    config={\"tools\": [{\"url_context\": {}}]}\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "bWtyuQtzs_iF",
        "outputId": "5130687b-0470-47c4-fa81-ad0b4d633459"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The James Webb Space Telescope (JWST) is equipped with four key scientific instruments:\n*   **NIRCam** (Near Infrared Camera) is an infrared imager covering a spectral range from 0.6 μm (visible red) to 5 μm (near-infrared). It also functions as the observatory's wavefront sensor, essential for aligning and focusing the main mirror segments.\n*   **NIRSpec** (Near Infrared Spectrograph) performs spectroscopy over the same wavelength range as NIRCam. It offers low-resolution, multi-object, and integral field unit/long-slit spectroscopy modes.\n*   **MIRI** (Mid-Infrared Instrument) measures the mid-to-long-infrared wavelength range from 5 to 27 μm. It includes both a mid-infrared camera and an imaging spectrometer and must be kept extremely cold (below 6 K) for optimal operation.\n*   **FGS/NIRISS** (Fine Guidance Sensor and Near Infrared Imager and Slitless Spectrograph) is used to stabilize the observatory's line-of-sight during scientific observations. The NIRISS module also provides astronomical imaging and spectroscopy in the 0.8 to 5 μm wavelength range. While often referred to together, the FGS is for support and NIRISS is a scientific instrument."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Mixing Grounding Methods (URL + Search)\n",
        "\n",
        "You can give the model multiple tools at once. Let's create a \"Fact-Checker\" that can both summarize a provided news article and use Google Search to find more recent information on the topic."
      ],
      "metadata": {
        "id": "f0mta5wntkJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ask a multi-step question that requires both tools\n",
        "prompt = \"\"\"\n",
        "First, please summarize the main points of the provided NASA article https://www.nasa.gov/news-release/nasa-isro-satellite-lifts-off-to-track-earths-changing-surfaces/.\n",
        "Then, use Google Search to see if there have been any new updates or announcements\n",
        "about the NISAR (NASA-ISRO Synthetic Aperture Radar) satellite in the days since that article was published.\n",
        "\"\"\"\n",
        "\n",
        "# Use both tools\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents = prompt,\n",
        "    config={\"tools\": [{\"url_context\": {}}, {\"google_search\": {}}]}\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "5ItKiaQztsU7",
        "outputId": "5ff90754-d2f6-4a7e-f74f-e9ffba84c2de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The NASA-ISRO Synthetic Aperture Radar (NISAR) satellite, a joint mission by NASA and the Indian Space Research Organisation (ISRO), successfully launched on July 30, 2025, from India. The satellite, carrying advanced L-band and S-band radar systems, will provide an unprecedented three-dimensional view of Earth's changing surfaces, monitoring land and ice movement, ecosystems, and natural disasters. NISAR will orbit 464 miles (747 kilometers) above Earth, observing nearly all land and ice-covered surfaces every 12 days, even through clouds and darkness, to provide crucial data for disaster response, infrastructure monitoring, and agricultural management. The mission is expected to have a 90-day commissioning phase, during which its 39-foot (12-meter) radar antenna reflector will deploy.\n\nNext, I will search for recent updates on the NISAR satellite.The NASA-ISRO Synthetic Aperture Radar (NISAR) satellite successfully launched on July 30, 2025, through a collaborative effort between NASA and the Indian Space Research Organisation (ISRO). The satellite's mission is to provide detailed, three-dimensional views of Earth's changing surfaces, tracking land and ice movement, monitoring ecosystems, and aiding in natural disaster response and agricultural management. It is equipped with advanced L-band and S-band radar systems, enabling it to \"see\" through clouds and darkness to observe nearly all land and ice-covered surfaces every 12 days from an orbit of 464 miles (747 kilometers) above Earth.\n\nSince its launch, the NISAR satellite has made significant progress in its commissioning phase. On August 15, 2025, its 39-foot (12-meter) radar antenna reflector was successfully deployed. Following this, engineers powered on both the L-band and S-band synthetic aperture radar systems. By August 26, the mission team commanded the spacecraft to begin ascending into its operational orbit. The satellite also sent back its first radar images of Earth's surface, which were released on September 25, 2025, showcasing its ability to scan Earth with high detail for various applications like disaster response, infrastructure monitoring, and agricultural management. These initial images, captured by the L-band SAR system, which was provided by NASA's Jet Propulsion Laboratory, demonstrate its capacity to resolve objects as small as 15 feet (5 meters). The mission is on track to commence full science operations later in the fall."
          },
          "metadata": {}
        }
      ]
    }
  ]
}